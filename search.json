[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Optimization for ML",
    "section": "",
    "text": "Optimization for ML\n\nCourse for 3rd year students of CS department at HSE university. 1 lecture + 1 seminar per week.\nThe course covers convex, non-convex, continuous optimization topics, especially motivated by problems and applications in Machine Learning. Various topics are covered, from fundamental materials to recent research.\n\n                        \n                                            \n\n\nTeam\n\n\n    \n        \n            \n                \n                  \n                    \n                  \n                  \n                    \n                      Daniil Merkulov\n                    \n                    Instructor\n                  \n                \n              \n        \n            \n                \n                  \n                    \n                  \n                  \n                    \n                      Farukh Yaushev\n                    \n                    Seminarist\n                  \n                \n              \n        \n            \n                \n                  \n                    \n                  \n                  \n                    \n                      Kirill Nikorov\n                    \n                    Seminarist\n                  \n                \n              \n        \n            \n                \n                  \n                    \n                  \n                  \n                    \n                      Michael Persiianov\n                    \n                    Seminarist\n                  \n                \n              \n        \n            \n                \n                  \n                    \n                  \n                  \n                    \n                      Arina Kosovskaia\n                    \n                    Assistant\n                  \n                \n              \n        \n            \n                \n                  \n                    \n                  \n                  \n                    \n                      Daniil Radushev\n                    \n                    Assistant\n                  \n                \n              \n        \n            \n                \n                  \n                    \n                  \n                  \n                    \n                      Maria Medvedeva\n                    \n                    Assistant\n                  \n                \n              \n        \n            \n                \n                  \n                    \n                  \n                  \n                    \n                      Alina Potemkina\n                    \n                    Assistant\n                  \n                \n              \n        \n            \n                \n                  \n                    \n                  \n                  \n                    \n                      Oleg Kurilov\n                    \n                    Assistant\n                  \n                \n              \n        \n            \n                \n                  \n                    \n                  \n                  \n                    \n                      Sabina\n                    \n                    Assistant\n                  \n                \n              \n        \n            \n                \n                  \n                    \n                  \n                  \n                    \n                      Svetlana Mikhailova\n                    \n                    Assistant\n                  \n                \n              \n        \n    \n\n\nNo matching items"
  },
  {
    "objectID": "program.html",
    "href": "program.html",
    "title": "",
    "section": "",
    "text": "Занятие 1\n    \n        \n    \n    Вспоминаем линейную алгебру. Некоторые матричные разложения. Скорость сходимости. Одномерная оптимизация. Неточная одномерная оптимизация.\n\n    Занятие 2\n    \n        \n    \n    Градиент. Гессиан. Матрично-векторное дифференцирование. Автоматическое дифференцирование. Вычислительный граф.\n\n    Занятие 3\n    \n        \n    \n    Выпуклость. Выпуклые множества. Выпуклые функции. Неравенство Йенсена. Сильно выпуклые функции. Условие Поляка - Лоясиевича.\n\n    Занятие 4\n    \n        \n    \n    Условия оптимальности. Функция Лагранжа. Множители Лагранжа. Теорема Каруша - Куна - Таккера.\n\n    Занятие 5\n    \n        \n    \n    Двойственность. Введение в двойственность. Двойственная задача. Анализ чувствительности.\n\n    Занятие 6\n    \n        \n    \n    Задача линейного программирования. Симплекс метод.\n\n    Занятие 7\n    \n        \n    \n    Градиентный спуск. Теоремы сходимости в гладком случае (выпуклые, сильно выпуклые, PL). Верхние и нижние оценки сходимости.\n\n    Занятие 8\n    \n        \n    \n    Ускоренные градиентные методы. Метод Поляка, Нестерова.\n\n    Занятие 9\n    \n        \n    \n    Субградиент. Субдифференциал. Субградиентный спуск. Теоремы сходимости в негладком случае. Особенности работы градиентного метода в практических негладких задачах.\n\n    Занятие 10\n    \n        \n    \n    Метод сопряженных градиентов.\n\n    Занятие 11\n    \n        \n    \n    Проксимальный градиентный метод. Метод Франк-Вульфа. Метод проекции градиента.\n\n    Занятие 12\n    \n        \n    \n    Метод натурального градиента. Метод зеркального спуска. K-FAC.\n\n    Занятие 13\n    \n        \n    \n    Метод Ньютона. Квазиньютоновские методы.\n\n    Занятие 14\n    \n        \n    \n    Введение в методы внутренней точки.\n\n    Занятие 15\n    \n        \n    \n    Стохастический градиентный спуск. Адаптивные стохастические градиентные алгоритмы: AdaDelta, RMSProp, Adam, Nadam.\n\n    Занятие 16\n    \n        \n    \n    Методы редукции дисперсии: SAG, SVRG, SAGA.\n\n    Занятие 17\n    \n        \n    \n    Градиентный поток и диффузия. Методы оптимизации в непрерывном времени.\n\n    Занятие 18\n    \n        \n    \n    Седловые задачи. Минимакс. Обучение GAN.\n\n    Занятие 19\n    \n        \n    \n    Обобщающая способность моделей машинного обучения. Neural Tangent Kernel. Mode connectivity.\n\n    Занятие 20\n    \n        \n    \n    Вопросы обучения больших моделей. Lars, Lamb. Расписания learning rate. Warm-up. Клиппинг.\n\n\nNo matching items"
  }
]