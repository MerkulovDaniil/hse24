[
  {
    "objectID": "program.html",
    "href": "program.html",
    "title": "",
    "section": "",
    "text": "Занятие 1\n    \n        \n    \n    Вспоминаем линейную алгебру. Некоторые матричные разложения. Скорость сходимости. Одномерная оптимизация. Неточная одномерная оптимизация.\n\n    Занятие 2\n    \n        \n    \n    Градиент. Гессиан. Матрично-векторное дифференцирование. Автоматическое дифференцирование. Вычислительный граф.\n\n    Занятие 3\n    \n        \n    \n    Выпуклость. Выпуклые множества. Выпуклые функции. Неравенство Йенсена. Сильно выпуклые функции. Условие Поляка - Лоясиевича.\n\n    Занятие 4\n    \n        \n    \n    Условия оптимальности. Функция Лагранжа. Множители Лагранжа. Теорема Каруша - Куна - Таккера.\n\n    Занятие 5\n    \n        \n    \n    Двойственность. Введение в двойственность. Двойственная задача. Анализ чувствительности.\n\n    Занятие 6\n    \n        \n    \n    Задача линейного программирования. Симплекс метод.\n\n    Занятие 7\n    \n        \n    \n    Градиентный спуск. Теоремы сходимости в гладком случае (выпуклые, сильно выпуклые, PL). Верхние и нижние оценки сходимости.\n\n    Занятие 8\n    \n        \n    \n    Ускоренные градиентные методы. Метод Поляка, Нестерова.\n\n    Занятие 9\n    \n        \n    \n    Субградиент. Субдифференциал. Субградиентный спуск. Теоремы сходимости в негладком случае. Особенности работы градиентного метода в практических негладких задачах.\n\n    Занятие 10\n    \n        \n    \n    Метод сопряженных градиентов.\n\n    Занятие 11\n    \n        \n    \n    Проксимальный градиентный метод. Метод Франк-Вульфа. Метод проекции градиента.\n\n    Занятие 12\n    \n        \n    \n    Метод натурального градиента. Метод зеркального спуска. K-FAC.\n\n    Занятие 13\n    \n        \n    \n    Метод Ньютона. Квазиньютоновские методы.\n\n    Занятие 14\n    \n        \n    \n    Введение в методы внутренней точки.\n\n    Занятие 15\n    \n        \n    \n    Стохастический градиентный спуск. Адаптивные стохастические градиентные алгоритмы: AdaDelta, RMSProp, Adam, Nadam.\n\n    Занятие 16\n    \n        \n    \n    Методы редукции дисперсии: SAG, SVRG, SAGA.\n\n    Занятие 17\n    \n        \n    \n    Градиентный поток и диффузия. Методы оптимизации в непрерывном времени.\n\n    Занятие 18\n    \n        \n    \n    Седловые задачи. Минимакс. Обучение GAN.\n\n    Занятие 19\n    \n        \n    \n    Обобщающая способность моделей машинного обучения. Neural Tangent Kernel. Mode connectivity.\n\n    Занятие 20\n    \n        \n    \n    Вопросы обучения больших моделей. Lars, Lamb. Расписания learning rate. Warm-up. Клиппинг.\n\n\nNo matching items"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Методы оптимизации в машинном обучении. МФТИ 2023-2024",
    "section": "",
    "text": "Методы оптимизации в машинном обучении. МФТИ 2023-2024\n\nЛектор - Меркулов Даниил Максимович (Сколтех, МФТИ, ВШЭ)\nКурс для студентов 3 курса ПМИ ФКН ВШЭ. 1 лекция + 1 семинар в неделю.\nСтарт учебной недели - 8 января. Длительность 20 недель.\nКурс охватывает темы выпуклой, невыпуклой, непрерывной оптимизации, особенно мотивированные задачами и приложениями в Машинном Обучении. Рассматриваются разные темы - от фундаментальных материалов до недавних исследований."
  }
]